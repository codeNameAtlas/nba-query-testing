{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd240556-f0f7-4f9b-9d9f-c6da7a27dd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBA Natural Language to SQL Testing Notebook\n",
    "# ============================================\n",
    "#\n",
    "# This notebook tests Claude's ability to convert natural language queries about NBA data\n",
    "# into SQL queries that match the expected results from ground truth queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceda6522-1d35-46ba-9d9a-168349fb8deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup and Imports ---\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import sqlite3\n",
    "import re\n",
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Load environment variables (API key)\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Anthropic client\n",
    "client = anthropic.Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "923dbfb3-ad1d-43f3-80ce-afe165d89c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Database Connection ---\n",
    "\n",
    "# Update this path to your SQLite database file\n",
    "DB_PATH = \"nba_database.sqlite\"\n",
    "\n",
    "def connect_to_db():\n",
    "    \"\"\"Connect to the SQLite database and return connection object.\"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(DB_PATH)\n",
    "        print(\"‚úÖ Connected to database successfully\")\n",
    "        return conn\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"‚ùå Error connecting to database: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b60fb71-4f07-4b5a-892e-c23509f4d2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ground Truth Data Loading ---\n",
    "\n",
    "def load_ground_truth_data(json_path=\"ground_truth_data.json\"):\n",
    "    \"\"\"Load ground truth data from JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"‚úÖ Loaded {len(data)} ground truth examples\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading ground truth data: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad7dd662-4a2e-435c-8ddd-d6ae28941019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Query Execution Functions ---\n",
    "\n",
    "def execute_sql_query(conn, query):\n",
    "    \"\"\"Execute SQL query and return results as a list of tuples.\"\"\"\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        \n",
    "        # Get column names\n",
    "        column_names = [description[0] for description in cursor.description]\n",
    "        \n",
    "        # Fetch all rows\n",
    "        rows = cursor.fetchall()\n",
    "        \n",
    "        return {\n",
    "            'column_names': column_names,\n",
    "            'rows': rows\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error executing query: {e}\")\n",
    "        print(f\"Query was: {query}\")\n",
    "        return None\n",
    "\n",
    "def compare_query_results(ground_truth_results, generated_results):\n",
    "    \"\"\"\n",
    "    Compare results from ground truth and generated queries.\n",
    "    Returns True if results match, False otherwise.\n",
    "    Focus on content rather than structure.\n",
    "    \"\"\"\n",
    "    if ground_truth_results is None or generated_results is None:\n",
    "        return False\n",
    "    \n",
    "    # If either result set is empty, both should be empty\n",
    "    if len(ground_truth_results['rows']) == 0:\n",
    "        return len(generated_results['rows']) == 0\n",
    "    \n",
    "    # Handle single row, single value results (most common case)\n",
    "    if len(ground_truth_results['rows']) == 1 and len(ground_truth_results['column_names']) == 1:\n",
    "        gt_val = ground_truth_results['rows'][0][0]\n",
    "        \n",
    "        # Generated result might have multiple columns, but we just check first column of first row\n",
    "        if len(generated_results['rows']) > 0 and len(generated_results['rows'][0]) > 0:\n",
    "            gen_val = generated_results['rows'][0][0]\n",
    "            \n",
    "            # For numeric values, check with tolerance\n",
    "            if isinstance(gt_val, (int, float)) and isinstance(gen_val, (int, float)):\n",
    "                return abs(gt_val - gen_val) < 0.01\n",
    "            # For strings or other types, check exact match\n",
    "            return gt_val == gen_val\n",
    "        return False\n",
    "    \n",
    "    # For multi-row results, convert to sets of tuples for comparison (ignores order)\n",
    "    # Only compare the first column if ground truth has just one column\n",
    "    if len(ground_truth_results['column_names']) == 1:\n",
    "        gt_set = set(row[0] for row in ground_truth_results['rows'])\n",
    "        \n",
    "        # Generated result might have multiple columns, but we just check first column\n",
    "        gen_set = set(row[0] for row in generated_results['rows'] if len(row) > 0)\n",
    "        \n",
    "        # If we're looking at a sample (first few rows only), just check these are in generated results\n",
    "        if len(ground_truth_results['rows']) <= 5:\n",
    "            return all(val in gen_set for val in gt_set)\n",
    "        \n",
    "        # Otherwise, check the sets are the same\n",
    "        return gt_set == gen_set\n",
    "    \n",
    "    # If multiple columns, convert to sets of tuples\n",
    "    gt_set = set(tuple(row) for row in ground_truth_results['rows'])\n",
    "    gen_set = set(tuple(row) for row in generated_results['rows'])\n",
    "    \n",
    "    # If we're looking at a sample (first few rows only), check these are in generated results\n",
    "    if len(ground_truth_results['rows']) <= 5:\n",
    "        return all(row in gen_set for row in gt_set)\n",
    "    \n",
    "    # Otherwise compare the full sets\n",
    "    return gt_set == gen_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "500834d2-ef88-41ce-b654-85813e902c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Claude API Interaction with Feedback ---\n",
    "\n",
    "def get_sql_from_claude_with_feedback(question, expected_sql=None):\n",
    "    \"\"\"Get SQL query from Claude for the given natural language question with feedback.\"\"\"\n",
    "    feedback_mode = expected_sql is not None\n",
    "    \n",
    "    # Build the prompt\n",
    "    prompt_text = \"\"\"You are an AI assistant tasked with converting natural language queries about the NBA into SQL queries. You will be provided with a database schema to help you understand the structure of the data and formulate correct SQL queries.\n",
    "\n",
    "<schema>\n",
    "Database structure:\n",
    "   - game: game_id, team_id_home, team_name_home, team_id_away, team_name_away, pts_home, pts_away, season_type, fg3m_home, fg3m_away, fg3a_home, fg3a_away, ftm_home, ftm_away, fta_home, fta_away, ast_home, ast_away, reb_home, reb_away, oreb_home, oreb_away, dreb_home, dreb_away, blk_home, blk_away, stl_home, stl_away, tov_home, tov_away, pf_home, pf_away\n",
    "   - team: id, full_name, abbreviation, nickname, city, state, year_founded\n",
    "   - player: id, full_name, first_name, last_name, is_active\n",
    "   - common_player_info: person_id, first_name, last_name, position, height, weight, country, jersey, team_id, season_exp, school\n",
    "   - game_info: game_id, game_date, attendance, game_time\n",
    "   - line_score: game_id, team_id_home, team_id_away, pts_ot1_home, pts_ot1_away, pts_home, pts_away\n",
    "   - draft_history: person_id, player_name, season, round_number, overall_pick, organization, organization_type\n",
    "   - other_stats: game_id, team_id_home, team_id_away, lead_changes, pts_paint_home, pts_paint_away, pts_fb_home, pts_fb_away\n",
    "   - inactive_players: player_id, first_name, last_name, team_id, game_id\n",
    "   - team_details: team_id, arena, arenacapacity\n",
    "   - team_history: team_id, city, nickname, year_founded, year_active_till\n",
    "</schema>\n",
    "\n",
    "<key_relationships>\n",
    "- game.team_id_home ‚Üí team.id\n",
    "- game.team_id_away ‚Üí team.id\n",
    "- common_player_info.person_id ‚Üí player.id\n",
    "- common_player_info.team_id ‚Üí team.id\n",
    "- game_info.game_id ‚Üí game.game_id\n",
    "- line_score.game_id ‚Üí game.game_id\n",
    "- other_stats.game_id ‚Üí game.game_id\n",
    "- inactive_players.team_id ‚Üí team.id\n",
    "- inactive_players.game_id ‚Üí game.game_id\n",
    "- team_details.team_id ‚Üí team.id\n",
    "- team_history.team_id ‚Üí team.id\n",
    "</key_relationships>\n",
    "\n",
    "<query>{0}</query>\n",
    "\n",
    "Please analyze the query and think through how to convert it into SQL. Consider the following:\n",
    "1. Which table(s) in the schema are relevant to this query?\n",
    "2. What columns need to be selected? Do not create new columns.\n",
    "3. Are any aggregations or groupings required?\n",
    "4. Are there any conditions that need to be applied (WHERE clause)?\n",
    "5. Is there a limit on the number of results to return?\n",
    "\n",
    "Before answering, here are some examples. You can see there is a \"natural language\" field, and a \"sql\" field.\n",
    "\n",
    "<example>\n",
    "[\n",
    "  {{\n",
    "    \"natural_language\": \"How many teams are currently in the NBA?\",\n",
    "    \"sql\": \"SELECT COUNT(*) as team_count FROM team LIMIT 1\",\n",
    "    \"type\": \"counting\"\n",
    "  }},\n",
    "  {{\n",
    "    \"natural_language\": \"List all teams from Texas.\",\n",
    "    \"sql\": \"SELECT full_name FROM team WHERE state = 'Texas'\",\n",
    "    \"type\": \"filtering\"\n",
    "  }},\n",
    "  {{\n",
    "    \"natural_language\": \"What's the lowest scoring game?\",\n",
    "    \"sql\": \"SELECT g.pts_home + g.pts_away as total_points FROM game g ORDER BY total_points ASC LIMIT 1\",\n",
    "    \"type\": \"ranking\"\n",
    "  }},\n",
    "  {{\n",
    "    \"natural_language\": \"Which team has the most away games?\",\n",
    "    \"sql\": \"SELECT t.full_name FROM game g JOIN team t ON g.team_id_away = t.id GROUP BY t.id, t.full_name ORDER BY COUNT(*) DESC LIMIT 1\",\n",
    "    \"type\": \"ranking\"\n",
    "  }},\n",
    "  {{\n",
    "    \"natural_language\": \"List all players from France.\",\n",
    "    \"sql\": \"SELECT first_name, last_name FROM common_player_info WHERE country = 'France'\",\n",
    "    \"type\": \"filtering\"\n",
    "  }},\n",
    "  {{\n",
    "    \"natural_language\": \"What's the most common jersey number above 10?\",\n",
    "    \"sql\": \"SELECT jersey FROM common_player_info WHERE CAST(jersey AS INTEGER) > 10 GROUP BY jersey ORDER BY COUNT(*) DESC LIMIT 1\",\n",
    "    \"type\": \"ranking\"\n",
    "  }},\n",
    "  {{\n",
    "    \"natural_language\": \"What's the average weight of NBA players?\",\n",
    "    \"sql\": \"SELECT ROUND(AVG(CAST(weight AS FLOAT)), 2) as avg_weight FROM common_player_info WHERE weight != '' LIMIT 1\",\n",
    "    \"type\": \"aggregation\"\n",
    "  }},\n",
    "  {{\n",
    "    \"natural_language\": \"Which team has the oldest arena?\",\n",
    "    \"sql\": \"SELECT t.full_name FROM team t JOIN team_details td ON t.id = td.team_id ORDER BY td.arena ASC LIMIT 1\",\n",
    "    \"type\": \"detail\"\n",
    "  }},\n",
    "  {{\n",
    "    \"natural_language\": \"List all second-year players.\",\n",
    "    \"sql\": \"SELECT first_name, last_name FROM common_player_info WHERE season_exp = 1\",\n",
    "    \"type\": \"filtering\"\n",
    "  }},\n",
    "  {{\n",
    "    \"natural_language\": \"What's the most points scored by the away team?\",\n",
    "    \"sql\": \"SELECT pts_away FROM game ORDER BY pts_away DESC LIMIT 1\",\n",
    "    \"type\": \"ranking\"\n",
    "  }},\n",
    "  {{\n",
    "    \"natural_language\": \"How many players are forwards?\",\n",
    "    \"sql\": \"SELECT COUNT(*) as forward_count FROM common_player_info WHERE position LIKE '%F%' LIMIT 1\",\n",
    "    \"type\": \"counting\"\n",
    "  }},\n",
    "  {{\n",
    "    \"natural_language\": \"List all games where both teams scored over 100 points.\",\n",
    "    \"sql\": \"SELECT g.game_id FROM game g WHERE g.pts_home > 100 AND g.pts_away > 100\",\n",
    "    \"type\": \"filtering\"\n",
    "  }},\n",
    "  {{\n",
    "    \"natural_language\": \"What's the most common height among NBA players?\",\n",
    "    \"sql\": \"SELECT height FROM common_player_info WHERE height != '' GROUP BY height ORDER BY COUNT(*) DESC LIMIT 1\",\n",
    "    \"type\": \"aggregation\"\n",
    "  }}\n",
    "]\n",
    "</example>\"\"\"\n",
    "\n",
    "    # Add feedback section if expected SQL is provided\n",
    "    if feedback_mode:\n",
    "        prompt_text += \"\"\"\n",
    "Now, based on your analysis, please provide the SQL query that would answer this natural language question. Write your SQL query inside <sql_query> tags.\n",
    "\n",
    "Now, let's compare your SQL query to the expected SQL query:\n",
    "\n",
    "Expected SQL:\n",
    "<expected_sql>\n",
    "{1}\n",
    "</expected_sql>\n",
    "\n",
    "When you reply, first plan on how you should answer within <thinking> </thinking>. This is a place to write down relevant content and will not be shown to the user. \n",
    "\n",
    "Once you are done thinking, output your final answer to the user within <answer> </answer>. Make sure your answer is formatted exactly as described. If the queries do not match, please provide feedback within <feedback></feedback> tags if the queries are similar, would return the same result, and have similar efficiency. If the queries match identically, then no feedback is necessary, but please still output the response that we have an exact match.\n",
    "\"\"\"\n",
    "    else:\n",
    "        prompt_text += \"\"\"\n",
    "Now, based on your analysis, please provide the SQL query that would answer this natural language question. Write your SQL query inside <sql_query> tags.\n",
    "\n",
    "When you reply, first plan on how you should answer within <thinking> </thinking>. This is a place to write down relevant content and will not be shown to the user. \n",
    "\n",
    "Once you are done thinking, output your final answer to the user within <answer> </answer>. Make sure your answer is formatted exactly as described.\n",
    "\"\"\"\n",
    "\n",
    "    # Format the prompt\n",
    "    if feedback_mode:\n",
    "        formatted_prompt = prompt_text.format(question, expected_sql)\n",
    "    else:\n",
    "        formatted_prompt = prompt_text.format(question)\n",
    "    \n",
    "    try:\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-opus-20240229\",\n",
    "            max_tokens=1000,\n",
    "            messages=[{\"role\": \"user\", \"content\": formatted_prompt}]\n",
    "        )\n",
    "        \n",
    "        # Extract Claude's response\n",
    "        answer = response.content[0].text\n",
    "        \n",
    "        # Extract SQL and feedback sections\n",
    "        sql_match = re.search(r'<sql_query>(.*?)</sql_query>', answer, re.DOTALL)\n",
    "        feedback_match = re.search(r'<feedback>(.*?)</feedback>', answer, re.DOTALL)\n",
    "        \n",
    "        sql = sql_match.group(1).strip() if sql_match else \"\"\n",
    "        feedback = feedback_match.group(1).strip() if feedback_match else \"\"\n",
    "        \n",
    "        return sql, feedback\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error calling Claude API: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b425f98-314c-4c3d-a21d-0281fbdf1f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Testing Functions ---\n",
    "\n",
    "def test_single_query(conn, ground_truth_item, use_feedback=False, verbose=True):\n",
    "    \"\"\"\n",
    "    Test a single natural language query against the ground truth.\n",
    "    Returns a dictionary with test results.\n",
    "    \"\"\"\n",
    "    question = ground_truth_item[\"natural_language\"]\n",
    "    ground_truth_sql = ground_truth_item[\"sql\"]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"üîç Testing: {question}\")\n",
    "    \n",
    "    # Get SQL from Claude - with or without feedback\n",
    "    if use_feedback:\n",
    "        claude_sql, feedback = get_sql_from_claude_with_feedback(question, ground_truth_sql)\n",
    "    else:\n",
    "        claude_sql, _ = get_sql_from_claude_with_feedback(question)\n",
    "    \n",
    "    if claude_sql is None:\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"ground_truth_sql\": ground_truth_sql,\n",
    "            \"claude_sql\": None,\n",
    "            \"success\": False,\n",
    "            \"error\": \"Failed to get SQL from Claude\"\n",
    "        }\n",
    "    \n",
    "    # Execute both queries\n",
    "    ground_truth_results = execute_sql_query(conn, ground_truth_sql)\n",
    "    claude_results = execute_sql_query(conn, claude_sql)\n",
    "    \n",
    "    # Compare results\n",
    "    success = compare_query_results(ground_truth_results, claude_results)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Ground Truth SQL: {ground_truth_sql}\")\n",
    "        print(f\"Claude's SQL: {claude_sql}\")\n",
    "        print(f\"Success: {'‚úÖ' if success else '‚ùå'}\")\n",
    "        \n",
    "        if use_feedback and feedback:\n",
    "            print(f\"Feedback: {feedback}\")\n",
    "            \n",
    "        if not success and ground_truth_results is not None and claude_results is not None:\n",
    "            print(\"\\nGround Truth Results:\")\n",
    "            print(f\"Columns: {ground_truth_results['column_names']}\")\n",
    "            for i, row in enumerate(ground_truth_results['rows'][:5]):  # Show up to 5 rows\n",
    "                print(f\"Row {i+1}: {row}\")\n",
    "                \n",
    "            print(\"\\nClaude Results:\")\n",
    "            print(f\"Columns: {claude_results['column_names']}\")\n",
    "            for i, row in enumerate(claude_results['rows'][:5]):  # Show up to 5 rows\n",
    "                print(f\"Row {i+1}: {row}\")\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"ground_truth_sql\": ground_truth_sql,\n",
    "        \"claude_sql\": claude_sql,\n",
    "        \"success\": success,\n",
    "        \"feedback\": feedback if feedback and use_feedback else None\n",
    "    }\n",
    "\n",
    "def run_example_tests(conn, ground_truth_data, num_examples=5, use_feedback=False):\n",
    "    \"\"\"Run tests on a random sample of examples from ground truth data.\"\"\"\n",
    "    if len(ground_truth_data) == 0:\n",
    "        print(\"‚ùå No ground truth data available.\")\n",
    "        return\n",
    "    \n",
    "    # Select random examples\n",
    "    examples = random.sample(ground_truth_data, min(num_examples, len(ground_truth_data)))\n",
    "    \n",
    "    results = []\n",
    "    for example in examples:\n",
    "        result = test_single_query(conn, example, use_feedback=use_feedback)\n",
    "        results.append(result)\n",
    "    \n",
    "    # Display summary\n",
    "    successes = sum(1 for r in results if r[\"success\"])\n",
    "    print(f\"\\nüìä Summary: {successes}/{len(results)} tests passed ({successes/len(results)*100:.1f}%)\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b53207d-1513-4f18-b34b-bd54153caa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 98 ground truth examples\n",
      "‚úÖ Connected to database successfully\n",
      "\n",
      "=== Testing with feedback mechanism ===\n",
      "üîç Testing: What's the average game attendance?\n",
      "Ground Truth SQL: SELECT ROUND(AVG(attendance), 0) as avg_attendance FROM game_info WHERE attendance > 0 LIMIT 1\n",
      "Claude's SQL: SELECT ROUND(AVG(attendance), 0) as avg_attendance \n",
      "FROM game_info\n",
      "WHERE attendance > 0\n",
      "LIMIT 1\n",
      "Success: ‚úÖ\n",
      "--------------------------------------------------\n",
      "üîç Testing: What's the average points per game?\n",
      "Ground Truth SQL: SELECT ROUND(AVG(pts_home + pts_away) / 2, 2) as avg_points FROM game LIMIT 1\n",
      "Claude's SQL: SELECT ROUND(AVG((pts_home + pts_away) / 2), 2) as avg_points FROM game LIMIT 1\n",
      "Success: ‚úÖ\n",
      "--------------------------------------------------\n",
      "üîç Testing: Which season type has the highest average attendance?\n",
      "Ground Truth SQL: SELECT g.season_type, ROUND(AVG(gi.attendance), 0) as avg_attendance FROM game g JOIN game_info gi ON g.game_id = gi.game_id WHERE gi.attendance > 0 GROUP BY g.season_type ORDER BY avg_attendance DESC LIMIT 1\n",
      "Claude's SQL: SELECT g.season_type, AVG(gi.attendance) as avg_attendance\n",
      "FROM game g \n",
      "JOIN game_info gi ON g.game_id = gi.game_id\n",
      "WHERE gi.attendance > 0\n",
      "GROUP BY g.season_type\n",
      "ORDER BY avg_attendance DESC \n",
      "LIMIT 1;\n",
      "Success: ‚ùå\n",
      "Feedback: The queries are very similar and should return the same result. The only difference is I did not round the average attendance in my query. Rounding is a good idea for cleaner output. The queries have the same joins, filtering, grouping, ordering and limit, so efficiency should be identical.\n",
      "\n",
      "Ground Truth Results:\n",
      "Columns: ['season_type', 'avg_attendance']\n",
      "Row 1: ('All-Star', 24658.0)\n",
      "\n",
      "Claude Results:\n",
      "Columns: ['season_type', 'avg_attendance']\n",
      "Row 1: ('All-Star', 24658.32)\n",
      "--------------------------------------------------\n",
      "üîç Testing: What's the average number of rebounds per game?\n",
      "Ground Truth SQL: SELECT ROUND(AVG(reb_home + reb_away), 2) as avg_rebounds FROM game\n",
      "Claude's SQL: SELECT ROUND(AVG(reb_home + reb_away), 2) as avg_rebounds \n",
      "FROM game\n",
      "Success: ‚úÖ\n",
      "--------------------------------------------------\n",
      "üîç Testing: What's the most points scored by the home team?\n",
      "Ground Truth SQL: SELECT pts_home FROM game ORDER BY pts_home DESC LIMIT 1\n",
      "Claude's SQL: SELECT pts_home FROM game ORDER BY pts_home DESC LIMIT 1\n",
      "Success: ‚úÖ\n",
      "--------------------------------------------------\n",
      "\n",
      "üìä Summary: 4/5 tests passed (80.0%)\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution ---\n",
    "\n",
    "# Load ground truth data\n",
    "ground_truth_data = load_ground_truth_data()\n",
    "\n",
    "# Connect to database\n",
    "conn = connect_to_db()\n",
    "\n",
    "if conn and ground_truth_data:\n",
    "    # Run tests on 5 random examples with feedback\n",
    "    print(\"\\n=== Testing with feedback mechanism ===\")\n",
    "    feedback_results = run_example_tests(conn, ground_truth_data, num_examples=5, use_feedback=True)\n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed with testing due to setup errors.\")\n",
    "\n",
    "# Close the database connection\n",
    "if conn:\n",
    "    conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
